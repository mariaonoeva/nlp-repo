{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# My spaCy, nltk and natasha notebook\n",
    "\n",
    "Hi, this is a notebook where I practice all my NLP skills. I use the first Harry Potter book in Russian as a data set. "
   ],
   "id": "94631c47a02e295c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:41:43.624721Z",
     "start_time": "2025-05-27T08:41:42.557598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(\"/Users/maria.onoeva/Desktop/new_folder/GitHub/nlp-repo\")"
   ],
   "id": "886bc695ab4d968e",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T12:48:01.831673Z",
     "start_time": "2025-05-27T12:48:01.649007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = 'questions/txts/'\n",
    "all_HP = '/Users/maria.onoeva/Desktop/HP_all_ru_Spivak.txt'\n",
    "\n",
    "with open(all_HP, encoding='utf8') as file:\n",
    "    text_ru = file.read()\n",
    "\n",
    "# cleaning text, not necessary for all\n",
    "\n",
    "#text_ru_cleaned = text_ru_cleaned.replace('\\n\\n', '') #  char\n",
    "\n",
    "import re\n",
    "text_ru_cleaned = re.sub(r'\\.(?!\\s)', '. ', text_ru)\n",
    "\n",
    "# Replace CRLF and other common line breaks with a space\n",
    "text_ru_cleaned = text_ru_cleaned.replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "# Replace actual non-breaking space (Unicode \\u00A0), not the string 'NBSP'\n",
    "text_ru_cleaned = text_ru_cleaned.replace('\\u00A0', ' ')\n",
    "\n",
    "text_ru_cleaned = re.sub(r'\\s{2,}', ' ', text_ru_cleaned)\n",
    "text_ru_cleaned = text_ru_cleaned.strip()"
   ],
   "id": "15f2baecec7d525f",
   "outputs": [],
   "execution_count": 207
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I import Russian from spaCy and initialize it in `nlp`. It then creates a so-called `doc` object. Docs contain tokens and if I call `doc[34]`, it will return the 34th token in this doc. There is also a `span` object as below.",
   "id": "58aa8c8bff83806d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:46:30.847234Z",
     "start_time": "2025-05-27T13:45:41.661355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "questions_spacy = []\n",
    "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"morphologizer\", \"tok2vec\", \"tagger\", \"attribute_ruler\", \"lemmatizer\", \"ner\", \"custom\", \"textcat\"])\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "nlp.max_length = 56065230\n",
    "doc = nlp(text_ru_cleaned)"
   ],
   "id": "54d4810f81a37423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parser']\n"
     ]
    }
   ],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:49:03.459259Z",
     "start_time": "2025-05-27T13:49:02.515170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "questions_spacy = [sent for sent in doc.sents if '?' in sent.text]\n",
    "print(questions_spacy)"
   ],
   "id": "92149e3a876b61d5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 255
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I want to print out some sentences. Since I investigate questions, I want to print out some of them. The first step is to extract all questions or sentences with a question mark at the end. Update: now I check whether sent contains '?' because I can have '??' (well this is ok with the previous method) or '?!'.",
   "id": "be297a82738c90e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I am not completely happy about automatic sentencing by `spaCy`. I'll try `nltk`.",
   "id": "4e2aa7438e0e788e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:46:47.858594Z",
     "start_time": "2025-05-27T08:46:47.783093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# This is necessary because I had the error with loading nltk parts \n",
    "nltk.download('punkt_tab')\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ],
   "id": "a6fcbd57ee7a3a87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:11:16.352807Z",
     "start_time": "2025-05-27T13:11:15.415311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences_nltk = sent_tokenize(text_ru_cleaned, language='russian')\n",
    "question_pattern = '?'\n",
    "questions_nltk = [sent for sent in sentences_nltk if question_pattern in sent]"
   ],
   "id": "dae2d7897728f18d",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Not the best but seems better. I am going to try another tool, [Natasha](https://natasha.github.io/).",
   "id": "715d89c233e64c6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T12:48:24.884364Z",
     "start_time": "2025-05-27T12:48:18.606886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from natasha import Segmenter, Doc\n",
    "\n",
    "segmenter = Segmenter()\n",
    "natasha_doc = Doc(text_ru_cleaned)\n",
    "\n",
    "natasha_sentences = natasha_doc.segment(segmenter)\n",
    "questions_natasha = [sent.text for sent in natasha_doc.sents if question_pattern in sent.text]"
   ],
   "id": "4d5abeec02bccd30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11252\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And for fun, let's do `regex`. ",
   "id": "ad23926a8b0515c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:09:12.042448Z",
     "start_time": "2025-05-27T13:09:12.030482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'spaCy: {len(questions_spacy)}' )\n",
    "print(f'NLTK: {len(questions_nltk)}')\n",
    "print(f'Natasha: {len(questions_natasha)}')"
   ],
   "id": "96dade7351f17f82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 11324\n",
      "NLTK: 11263\n",
      "Natasha: 11252\n",
      "regex: 11324\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I want to compare questions from all three tools. ",
   "id": "2793e57afe0ab73f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:22:24.929682Z",
     "start_time": "2025-05-27T13:22:24.867992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import zip_longest\n",
    "import csv\n",
    "\n",
    "questions_3_tools = 'questions/csvs/questions_3_tools.csv'\n",
    "\n",
    "with open(questions_3_tools, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=';')\n",
    "    \n",
    "    # Write header row\n",
    "    writer.writerow(['ID', 'spaCy', 'NLTK', 'Natasha'])\n",
    "    \n",
    "    # Use enumerate to add an ID for each row, starting at 1\n",
    "    for id_num, (spacy, nltk, natasha) in enumerate(zip_longest(questions_spacy, questions_nltk, questions_natasha, fillvalue=''), start=1):\n",
    "        writer.writerow([id_num, spacy, nltk, natasha])"
   ],
   "id": "58a0ba2a501f30e3",
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then I would like to do some syntactic parsing of some questions. ",
   "id": "3d27b1f213747468"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6342f9d4fd85c08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

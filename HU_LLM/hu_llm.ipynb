{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HU-LLM-Text Mistral Large: PhonologyBench for Russian ",
   "id": "2e7f53ba3fc3550"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspired by [PhonologyBench: Evaluating Phonological Skills of Large Language Models](https://aclanthology.org/2024.knowllm-1.1/) (Suvarna et al., KnowLLM 2024). \n",
   "id": "a4d4f7ac16a539f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1: Counting syllables in words\n",
    "\n",
    "The idea is to test how good is HU-LLM in counting syllables in Russian words. I took words with different number of syllables from [this web](https://slogi.su/1). I expect it to be quite good in this task as there is a clear algorithm how to identify a syllable in a Russian word. \n",
    "\n",
    "The preprocessed files with words are located in `words_folder`. The code below: \n",
    "\n",
    "- reads each file from the folder\n",
    "- counts how many words are in each file \n",
    "- creates a big csv with all words and syllables count"
   ],
   "id": "9551cc99a28350ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:59:12.465334Z",
     "start_time": "2025-07-15T10:59:12.335061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dfs = {}\n",
    "\n",
    "folder_path = \"/Users/maria.onoeva/Desktop/new_folder/GitHub/nlp-repo/HU_LLM/words_folder\"\n",
    "\n",
    "# Loop through all .txt files in the folder as above\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            words = file.read().split(', ')\n",
    "            length = len(words)\n",
    "\n",
    "        print(f\"Words in {filename}: {length}\")\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(words, columns=['word'])\n",
    "\n",
    "        # Extract syllable count from filename (e.g., 'words_3.txt' -> 3)\n",
    "        syllable_count = int(''.join([d for d in filename if d.isdigit()]))\n",
    "\n",
    "        # Add the syllable count column\n",
    "        df['syllable'] = syllable_count\n",
    "\n",
    "        # Store in dictionary using filename without extension as key\n",
    "        key = os.path.splitext(filename)[0]\n",
    "        dfs[key] = df\n",
    "\n",
    "# combine all DataFrames into one big one\n",
    "combined_df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# saving as csv\n",
    "combined_df.to_csv('combined_df.csv') "
   ],
   "id": "38caae6d0dce531c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in words_8.txt: 208\n",
      "Words in words_9.txt: 144\n",
      "Words in words_4.txt: 286\n",
      "Words in words_5.txt: 278\n",
      "Words in words_7.txt: 257\n",
      "Words in words_6.txt: 272\n",
      "Words in words_2.txt: 296\n",
      "Words in words_3.txt: 298\n",
      "Words in words_1.txt: 270\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T10:59:17.091661Z",
     "start_time": "2025-07-15T10:59:17.050082Z"
    }
   },
   "cell_type": "code",
   "source": "combined_df.count()",
   "id": "adcbd37c2ef80bbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word        2309\n",
       "syllable    2309\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I create 4 random samples of 250 words from the big csv and prompt it to HU-LLM via its API (1000 in one batch seems to be too much). ",
   "id": "bbc08a34240c2d55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:01:04.318403Z",
     "start_time": "2025-07-15T14:01:04.288285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_df_1000 = combined_df.sample(n=1000, random_state=420)\n",
    "\n",
    "# Split into 4 approximately equal parts\n",
    "split_dfs = np.array_split(sample_df_1000, 4)\n",
    "\n",
    "# If you want lists instead of DataFrames:\n",
    "split_lists = [subdf['word'].tolist() for subdf in split_dfs]\n",
    "\n",
    "# Access the 4 random non-overlapping lists:\n",
    "list1, list2, list3, list4 = split_lists"
   ],
   "id": "173a680f7c45bee3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maria.onoeva/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:48:15.205066Z",
     "start_time": "2025-07-15T12:48:15.106924Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df_1000.to_csv('sample_df_1000.csv')",
   "id": "e2aaa2facbba0405",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T12:10:04.670518Z",
     "start_time": "2025-07-15T11:52:35.505801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "for i in split_lists: \n",
    "    client = Client(\"https://llm1-compute.cms.hu-berlin.de/\")\n",
    "    result_list = client.predict(\n",
    "\t\tparam_0=f\"Please count syllables in these Russian words {i}\",\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "    with open(\"sample_1000_result.txt\", \"a\") as file:\n",
    "        file.write(result_list)\n"
   ],
   "id": "c4ca28eb4df99a29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://llm1-compute.cms.hu-berlin.de/ ✔\n",
      "Loaded as API: https://llm1-compute.cms.hu-berlin.de/ ✔\n",
      "Loaded as API: https://llm1-compute.cms.hu-berlin.de/ ✔\n",
      "Loaded as API: https://llm1-compute.cms.hu-berlin.de/ ✔\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before assessing the results, I need to clean the result file `sample_1000_result.txt`. I manually removed all except results and saved to a new file `sample_1000_result_clean.txt`. Now I also need to remove numbers and replace `' - '` pattern with comma. Saving to `cleaned_output.csv`.",
   "id": "63291b1330a7ff27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:01:10.339244Z",
     "start_time": "2025-07-15T14:01:10.318481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "pattern1 = re.compile(r\"\\d+\\.\\s\") # this will remove a number from the beginning of the line\n",
    "pattern2 = re.compile(r\"\\s*-\\s*\") # replaces \" - \" pattern \n",
    "\n",
    "cleaned_rows = []\n",
    "\n",
    "with open(\"sample_1000_result_clean.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = pattern1.sub(\"\", line)\n",
    "        line = pattern2.sub(\",\", line)\n",
    "        cleaned_line = line.strip()\n",
    "        row = cleaned_line.split(\",\")  # split into columns\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "cleaned_output = pd.DataFrame(cleaned_rows)\n",
    "cleaned_output.to_csv(\"cleaned_output.csv\", index=False, header=False)"
   ],
   "id": "19750f763b13c08c",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:59:27.110005Z",
     "start_time": "2025-07-15T13:59:27.087587Z"
    }
   },
   "cell_type": "code",
   "source": "cleaned_output.count()",
   "id": "90f09f8a09201fe9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now finally, I can compare the output. ",
   "id": "9aaa0899b518a6ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:05:11.729468Z",
     "start_time": "2025-07-15T14:05:11.700164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# because the dfs were not combined properly, I had to drop their initial indices \n",
    "sample_df_1000 = sample_df_1000.reset_index(drop=True)\n",
    "cleaned_output = cleaned_output.reset_index(drop=True)\n",
    "\n",
    "ready_df = pd.concat([sample_df_1000, cleaned_output], ignore_index=True, axis=1) # combining for comparison"
   ],
   "id": "56b6719b36f92718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:14:20.620440Z",
     "start_time": "2025-07-15T14:14:20.574260Z"
    }
   },
   "cell_type": "code",
   "source": "ready_df[3] = ready_df[3].astype(int) # making the third column as int",
   "id": "4bc1e037c0d158dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:43:33.178136Z",
     "start_time": "2025-07-15T14:43:33.135067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ready_df[4] = ready_df[1] == ready_df[3] # comparing columns \n",
    "ready_df[5] = ready_df[0] == ready_df[2] # comparing columns "
   ],
   "id": "6a952d85af6ac492",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:38:37.124012Z",
     "start_time": "2025-07-15T14:38:37.080438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "false_ready_df = ready_df.loc[ready_df[4]!=True].copy()\n",
    "false_ready_df[6] = false_ready_df[1]-false_ready_df[3]\n",
    "false_ready_df.count()"
   ],
   "id": "e9f8c25b3b510ffd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    449\n",
       "1    449\n",
       "2    449\n",
       "3    449\n",
       "4    449\n",
       "5    449\n",
       "6    449\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Well, 449 wrong hits is too much!!! What is going on? Is it because those words are super infrequent? Now I want to know per each prompt how much was wrong.",
   "id": "14d6385264817969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T15:16:37.206544Z",
     "start_time": "2025-07-15T15:16:37.106568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_ready_df = np.array_split(ready_df, 4)\n",
    "attempt1 = split_ready_df[0][4].mean()\n",
    "attempt2 = split_ready_df[1][4].mean()\n",
    "attempt3 = split_ready_df[2][4].mean()\n",
    "attempt4 = split_ready_df[3][4].mean()\n",
    "\n",
    "print(attempt1, attempt2, attempt3, attempt4)"
   ],
   "id": "f402d4e7ab92e29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.624 0.484 0.572 0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maria.onoeva/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a4a1d1f79c02fe96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 97,
   "source": "false_ready_df.to_csv(\"false_ready_df.csv\", index=False, header=False)",
   "id": "1eca15376f76f9f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2: Counting syllables in sentences and stress marking\n",
    "\n",
    "Данные из акцентологического корпуса"
   ],
   "id": "5c4160c48b7458bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
